## Optimizer

- 权重初始化
- 优化器
- learning rate method 


深度学习优化的两个关注点
- 1.尽量求得更优的解
- 2.求解速度尽可能快



### Optimizer

- Gradient Descent
  - Batch gradient
  - Stochastic gradient descent
  - Mini-batch gradient descent
- Adaptive
  - Adgrad
  - ADAdelta
  - RMSprop
  - Adam



参考资料：
- [1] [7 tips to choose the best optimizer](https://towardsdatascience.com/7-tips-to-choose-the-best-optimizer-47bb9c1219e)
- [2] [Gentle Introduction to the Adam Optimization Algorithm for Deep Learning](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)
- [3] [Learning-Rate Annealing Methods for Deep Neural Networks](https://www.mdpi.com/2079-9292/10/16/2029/htm)
- [4] [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/index.html)



<br>

