## Optimizer

- 权重初始化
- 优化器
- learning rate method 


深度学习优化的两个关注点
- 1.尽量求得更优的解
- 2.求解速度尽可能快



### Optimizer

- Gradient Descent
  - Batch gradient
  - Stochastic gradient descent
  - Mini-batch gradient descent
- Adaptive
  - Adgrad
  - ADAdelta
  - RMSprop
  - Adam
  - Adamax
  - Nadam

### Optimizer

优化器：
- 不同优化其


---

### 优化方法（Optimization）

如何选择优化方法？
全局最优化，避免局部最优化

常见的优化方法有：
- SGD(stochastic gradient descent)
- Momentum
- Nesterov
- Adagrad
- Adadelta
- RMSprop
- Adam
- Adamax
- Nadam

更多资料，可以参考：
- [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/)
- [深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）](https://zhuanlan.zhihu.com/p/22252270)





### Learning rate



参考资料：
- [1] [7 tips to choose the best optimizer](https://towardsdatascience.com/7-tips-to-choose-the-best-optimizer-47bb9c1219e)
- [2] [Gentle Introduction to the Adam Optimization Algorithm for Deep Learning](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)
- [3] [Learning-Rate Annealing Methods for Deep Neural Networks](https://www.mdpi.com/2079-9292/10/16/2029/htm)
- [4] [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/index.html)



<br>

