<!DOCTYPE html>
<html>
<head>
<title>deep_learning_activation_function.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h2 id="%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">激活函数</h2>
<h3 id="%E7%AE%80%E4%BB%8B">简介</h3>
<blockquote>
<p>&quot;在人工神经网络中，激活函数是根据输入定义节点（或神经元）的输出。一般，该输出也会是下一个节点的输入。&quot; - <a href="https://en.wikipedia.org/wiki/Activation_function">Activation function</a></p>
</blockquote>
<p>为解决非线性问题（实际中大多数问题都是非线性问题），神经网络中引入了激活函数，给网络结构添加了非线性因素，使其能够处理较为复杂的非线性分类问题。</p>
<p>激活函数一般具有以下性质:</p>
<blockquote>
<ul>
<li>非线性: 当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。但是，如果激活函数是恒等激活函数的时候，就不满足这个性质了，而且如果多层网络使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。</li>
<li>连续可微: 当优化方法是基于梯度的时候，这个性质是必须的。</li>
<li>范围: 当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况下，一般需要更小的learning rate。</li>
<li>单调性: 当激活函数是单调的时候，单层网络能够保证是凸函数。</li>
<li>导数单调的光滑函数: 在某些情况下，这些已被证明具有更好的泛化性。</li>
<li>初始时 f(x)≈x: 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。<br>
- <a href="https://en.wikipedia.org/wiki/Activation_function">Activation function</a></li>
</ul>
</blockquote>
<p>想要了解更多，可以阅读：</p>
<blockquote>
<p><a href="http://www.shuang0420.com/2017/01/21/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%88%B0%E9%9D%9E%E7%BA%BF%E6%80%A7/">深度学习-从线性到非线性</a></p>
</blockquote>
<h3 id="%E5%B8%B8%E7%94%A8%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">常用的激活函数</h3>
<p>常用的激活函数有:</p>
<ul>
<li>Sigmoid</li>
<li>Tanh</li>
<li>ReLU</li>
<li>Leaky ReLU，PReLU、R-ReLU</li>
<li>ELU</li>
<li>Maxout</li>
<li>Softmax</li>
<li>其他</li>
</ul>
<h4 id="sigmoid">Sigmoid</h4>
<p>1844年（1845年发表） 就提出来了，经典的激活函数，但是目前使用的不太多。Sigmoid函数单调平滑可导，能够压缩输出数值，但Sigmoid函数左右两端饱和，梯度很小，容易导致梯度损失。除此之外，Sigmoid函数不是关于原点中心对称，函数中的幂运算也相对耗时。</p>
<p>
\sigma(x)=\frac{1}{1+e^{-x}
</p>
<center>
<img src="http://latex.codecogs.com/gif.latex?\sigma(x)=\frac{1}{1+e^{-x}}">
<br>
<img src='resource/deep_learning_activation_function/img_01.png' height=200>
</center>
<h4 id="tanh">Tanh</h4>
<p>与Sigmoid函数很类似，也是经典的激活函数。与Sigmoid函数相比，Tanh改进的地方是函数输出值关于原点中心对称。梯度损失和计算耗时的问题还是存在。</p>
<pre class="hljs"><code><div>f(x)=tanh (x)=\frac{(e^{x}-e^{-x})}{(e^{x}+e^{-x})}
</div></code></pre>
<center>
<img src="http://latex.codecogs.com/gif.latex?\sigma(x)=\frac{1}{1+e^{-x}}">
<br>
<img src='resource/deep_learning_activation_function/img_02.gif' height=220>
</center>
<h4 id="relu">ReLU</h4>
<p>ReLU 在深度学习中应用，是由 <em>Xavier Glorot et al. 2011</em> 在 <a href="http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf">Deep Sparse Rectifier Neural Networks</a> 中提出，其在深度学习上的表现一举超过sigmoid和tanh，并如作者所言，成为一个里程碑，后续深度学习网络大多都采用ReLU作为激活函数。ReLU函数计算复杂度，负类部分置为0，能够稀疏参数，加快收敛速度（SGD），但同时也会导致部分神经元“坏死”，权重无法更新。除此之外，ReLU没有对数据进行幅度压缩，输出也不是关于原点对称。使用ReLU激活，需要小心调整学习率（SGD）。</p>
<pre class="hljs"><code><div>f(x) = \left\{\begin{matrix}
0 \qquad if\, x &lt; 0\\ 
x \qquad if\, x\geqslant 0
\end{matrix}\right.
</div></code></pre>
<center>
<img src='resource/deep_learning_activation_function/img_03.png' height=260>
</center>
<h4 id="leaky-relu-parameteric-reluprelu-randomized-leaky-relurrelu">Leaky ReLU, Parameteric ReLU(PReLU), Randomized leaky ReLU(RReLU),</h4>
<p><em>Bing Xu et al. 2015</em> 在 <a href="https://arxiv.org/abs/1505.00853">Empirical Evaluation of Rectified Activations in Convolutional Network</a> 对这几种激活函数进行了分析比较， 他们通过实验比较了这几种ReLU激活函数对分类网络的影响，改善的ReLU比原始ReLU得到的结果要更好，其中RReLU更胜一筹，但是这个结果不一定适用于其他数据集和网络。但可以肯定的是，ReLU并不是激活函数的终结，还会有更合适的激活函数出现。</p>
<center>
<img src='resource/deep_learning_activation_function/img_04.png' height=230>
</center>
<h5 id="leaky-relu">Leaky ReLU</h5>
<p><em>Maas et al., 2013</em> 在 <a href="https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf">Rectifier Nonlinearities Improve Neural Network Acoustic Models</a> 中首次提出，比较的结果并没有明显的改善，具体可以阅读原论文。</p>
<pre class="hljs"><code><div>f(x) = \left\{\begin{matrix}
0.01x \quad \, if\, x &lt; 0\\ 
x \quad \quad \quad if\, x\geqslant 0
\end{matrix}\right.
</div></code></pre>
<h5 id="prelu">PReLU</h5>
<p>由 <em>Kaiming He et al. 2015</em> 在 <a href="https://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a> 中提出，作者指出这是深度学习算法识别准确率首次超过人类，具体内容可以参考原论文。</p>
<pre class="hljs"><code><div>f(y_{i}) = \left\{\begin{matrix}
y_{i} \quad \quad if\, y_{i}&gt; 0\\ 
a_{i}y_{i} \quad \ if\, y_{i}\leq 0
\end{matrix}\right.
</div></code></pre>
<h5 id="rrelu">RReLU</h5>
<p>据说是在 <em>Kaggle NDSB Competition</em> 中提出，训练时a随机（区间内），测试时取平均值。</p>
<pre class="hljs"><code><div>
y_{ji} = \left\{\begin{matrix}
x_{ji} \quad \quad if\, x_{ji} \geqslant 0\\ 
a_{ji}x_{ji} \quad if\, x_{ji} &lt; 0
\end{matrix}\right.

a_{ji} \sim U(l, u), l&lt;u \ and \ l,u\in [0, 1)
</div></code></pre>
<h4 id="elu">ELU</h4>
<p><em>Djork-Arne Clevert et al. 2016</em> 在 <a href="https://arxiv.org/abs/1511.07289">Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)</a> 提出，在作者的实验中，ELU收敛速度与分类性能表现都更好。ELU缓解了神经元“坏死”的问题，从而能够加快收敛。</p>
<pre class="hljs"><code><div>f(x) = \left\{\begin{matrix}
x \qquad \qquad \quad if\, x&gt; 0\\
\alpha(e^{x}-1) \quad \ \  if\, x\leq 0
\end{matrix}\right.
</div></code></pre>
<center>
<img src='resource/deep_learning_activation_function/img_05.png' height=200>
</center>
<h4 id="maxout">Maxout</h4>
<p><em>Ian J. Goodfellow et al. 2013</em> 在 <a href="https://arxiv.org/abs/1302.4389">Maxout Networks</a> 提出，作者在论文提到这种激活函数为了与 <em>dropout</em> 配合使用，效果不错。</p>
<pre class="hljs"><code><div>h_{i}(x) = \mathrm{max}\ z_{ij}
</div></code></pre>
<h4 id="softmax">Softmax</h4>
<p>softmax与上面的激活函数不同，一般用在网络最后一层，进行多分类。</p>
<blockquote>
<p>&quot;它能将一个含任意实数的K维向量“压缩”到另一个K维实向量中，使得每一个元素的范围都在(0,1)之间，并且所有元素的和为1。&quot; - <a href="https://en.wikipedia.org/wiki/Softmax_function">Softmax function</a></p>
</blockquote>
<pre class="hljs"><code><div>\sigma(z)_{j}=\frac{e^{z_{j}}}{\sum^{K}_{k=1}e^{z_{k}}} \qquad j=1, ..., K
</div></code></pre>
<h4 id="%E5%85%B6%E4%BB%96">其他</h4>
<p>激活函数大概有几百多种，其他的一些激活函数如 SoftSign, SoftPlus等可以参考维基 &quot;<a href="https://en.wikipedia.org/wiki/Activation_function">Activation function</a>&quot;，里面对这些激活函数做了简要的介绍。</p>
<p>关于常用激活函数的介绍，还可以阅读这篇博客:</p>
<ul>
<li><a href="https://isaacchanghau.github.io/post/activation_functions/">Activation Functions in Neural Networks</a></li>
</ul>
<p><em>Dviad</em> 将一些激活函数进行了可视化绘制，具体可以参考:</p>
<ul>
<li><a href="https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/">Visualising Activation Functions in Neural Networks</a></li>
</ul>
<h4 id="%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98">常见问题</h4>
<h5 id="%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1">梯度消失</h5>
<p>loss 函数对x求导（也就是梯度），可以理解为<code>$\frac{\partial _{loss}}{\partial _{x}} = \frac{\partial _{loss}}{\partial _{y}} \cdot \frac{\partial _{y}}{\partial _{x}}$</code>。 <code>$\frac{\partial _{y}}{\partial _{x}}$</code> 也就是激活函数的导数（梯度），如果该结果比较小，向后传递，便会导致梯度损失，甚至消失，训练参数无法得到更新。</p>
<h5 id="%E5%85%B3%E4%BA%8E%E5%8E%9F%E7%82%B9%E4%B8%AD%E5%BF%83%E4%B8%8D%E5%AF%B9%E7%A7%B0">关于原点中心不对称</h5>
<ul>
<li><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">lecun</a></li>
</ul>
<h4 id="%E5%B0%8F%E7%BB%93">小结</h4>
<h5 id="%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">如何选择激活函数？</h5>
<p>通常，一个网络中尽量使用一种激活函数。哪种激活函数更好，目前并没有明确的定论。</p>
<p>一般来说，可以先尝试ReLU，但是ReLU对学习率比较敏感，需要谨慎选择。
另外，ReLU的改进版Leaky ReLU、 PReLU、 Maxout等也可以尝试一下，改善效果不确定。</p>
<p>非要使用饱和类激活函数时，优先选择tanh，而不是sigmoid.</p>
<h4 id="%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">参考资料</h4>
<p>论文，书籍</p>
<ul>
<li><a href="http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf">Deep Sparse Rectifier Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1505.00853">Empirical Evaluation of Rectified Activations in Convolutional Network</a></li>
<li><a href="https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf">Rectifier Nonlinearities Improve Neural Network Acoustic Models</a></li>
<li><a href="https://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></li>
<li><a href="https://arxiv.org/abs/1511.07289">Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)</a></li>
<li><a href="https://arxiv.org/abs/1302.4389">Maxout Networks</a></li>
</ul>
<p>博客，维基</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Activation_function">Activation function</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sigmoid_function">Sigmoid function</a></li>
<li><a href="http://mathworld.wolfram.com/HyperbolicTangent.html">Hyperbolic Tangent</a></li>
<li><a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">Rectifier</a></li>
<li><a href="https://isaacchanghau.github.io/post/activation_functions/">Activation Functions in Neural Networks</a></li>
<li><a href="https://juejin.im/entry/58a1576e2f301e006952ded1">常用激活函数的总结与比较</a></li>
<li><a href="https://blog.csdn.net/NOT_GUY/article/details/78749509">深度学习中激活函数的优缺点</a></li>
<li><a href="http://www.shuang0420.com/2017/01/21/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%88%B0%E9%9D%9E%E7%BA%BF%E6%80%A7/">深度学习-从线性到非线性</a></li>
<li><a href="https://www.cnblogs.com/rgvb178/p/6055213.html">The Activation Function in Deep Learning 浅谈深度学习中的激活函数</a></li>
<li><a href="https://blog.csdn.net/u014595019/article/details/52562159">深度学习笔记(三)：激活函数和损失函数</a></li>
<li><a href="https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6">Activation Functions: Neural Networks</a></li>
<li><a href="https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/">Visualising Activation Functions in Neural Networks</a></li>
</ul>

</body>
</html>
